# ãƒ‰ãƒ¡ã‚¤ãƒ³é§†å‹•è¨­è¨ˆï¼ˆDDDï¼‰

## ğŸ“‹ æ–‡æ›¸æƒ…å ±

| é …ç›®       | å†…å®¹                    |
| ---------- | ----------------------- |
| æ–‡æ›¸å     | ãƒ‰ãƒ¡ã‚¤ãƒ³é§†å‹•è¨­è¨ˆï¼ˆDDDï¼‰ |
| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | 1.0                     |
| ä½œæˆæ—¥     | 2025 å¹´ 9 æœˆ 28 æ—¥      |
| ä½œæˆè€…     | AI é–‹ç™ºãƒãƒ¼ãƒ            |
| æ‰¿èªè€…     | æŠ€è¡“è²¬ä»»è€…              |
| ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ | è‰æ¡ˆ                    |

## ğŸ¯ æ¦‚è¦

Disclosure Evaluator ã§ã¯ã€ãƒ‰ãƒ¡ã‚¤ãƒ³é§†å‹•è¨­è¨ˆï¼ˆDomain-Driven Design, DDDï¼‰ã‚’æ¡ç”¨ã—ã€æƒ…å ±å…¬é–‹æ³•è©•ä¾¡ã¨ã„ã†è¤‡é›‘ãªãƒ“ã‚¸ãƒã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é©åˆ‡ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚

## ğŸ—ï¸ DDD æˆ¦ç•¥çš„è¨­è¨ˆ

### 1. å¢ƒç•Œã¥ã‘ã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

#### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒ—

```mermaid
graph TB
    subgraph "è©•ä¾¡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (Core Domain)"
        A[è©•ä¾¡å®Ÿè¡Œ]
        B[è©•ä¾¡çµæœç®¡ç†]
        C[æƒ…å ±å…¬é–‹æ³•è©•ä¾¡]
    end

    subgraph "ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (Supporting Domain)"
        D[ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç®¡ç†]
        E[APIçµ±åˆ]
    end

    subgraph "åˆ†æã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (Supporting Domain)"
        F[çµ±è¨ˆåˆ†æ]
        G[æ¯”è¼ƒåˆ†æ]
    end

    subgraph "ãƒ¬ãƒãƒ¼ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (Supporting Domain)"
        H[ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ]
        I[å¯è¦–åŒ–]
    end

    subgraph "å…±é€šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (Generic Domain)"
        J[è¨­å®šç®¡ç†]
        K[ãƒ­ã‚°ç®¡ç†]
        L[ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£]
    end

    A --> D
    A --> F
    A --> H
    B --> F
    C --> F
    D --> J
    E --> K
    F --> I
    G --> I
    H --> I
```

#### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜

| ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ                 | ã‚¿ã‚¤ãƒ—            | èª¬æ˜                           |
| ---------------------------- | ----------------- | ------------------------------ |
| **è©•ä¾¡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**         | Core Domain       | ã‚·ã‚¹ãƒ†ãƒ ã®æ ¸ã¨ãªã‚‹è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ |
| **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ** | Supporting Domain | LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çµ±åˆ           |
| **åˆ†æã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**         | Supporting Domain | è©•ä¾¡çµæœã®åˆ†æãƒ»æ¯”è¼ƒ           |
| **ãƒ¬ãƒãƒ¼ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**     | Supporting Domain | ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»å¯è¦–åŒ–           |
| **å…±é€šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**         | Generic Domain    | æ¨ªæ–­çš„é–¢å¿ƒäº‹                   |

### 2. ãƒ¦ãƒ“ã‚­ã‚¿ã‚¹è¨€èª

#### è©•ä¾¡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç”¨èª

| ç”¨èª               | å®šç¾©                                         | è‹±èª              |
| ------------------ | -------------------------------------------- | ----------------- |
| **è©•ä¾¡**           | LLM å‡ºåŠ›ã®å“è³ªã‚’æ¸¬å®šã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹             | Evaluation        |
| **è©•ä¾¡æŒ‡æ¨™**       | å“è³ªã‚’æ¸¬å®šã™ã‚‹åŸºæº–                           | Evaluation Metric |
| **è©•ä¾¡çµæœ**       | æ§‹é€ åŒ–ã•ã‚ŒãŸè©•ä¾¡ãƒ‡ãƒ¼ã‚¿                       | Evaluation Result |
| **è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³**   | è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ             | Evaluation Engine |
| **æ³•çš„è©•ä¾¡**       | æ³•çš„æ¡æ–‡ã«åŸºã¥ã 6 ã¤ã®ä¸é–‹ç¤ºäº‹ç”±ã®è©•ä¾¡      | Legal Evaluation  |
| **æ®µéšçš„è©•ä¾¡**     | 3-4 æ®µéšã®è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹                       | Staged Evaluation |
| **é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢** | è©•ä¾¡æŒ‡æ¨™ã®é‡è¦åº¦ã«å¿œã˜ãŸé‡ã¿ä»˜ã‘ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ | Weighted Score    |

#### ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç”¨èª

| ç”¨èª             | å®šç¾©                            | è‹±èª                  |
| ---------------- | ------------------------------- | --------------------- |
| **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼** | LLM ã‚µãƒ¼ãƒ“ã‚¹æä¾›è€…              | Provider              |
| **API è¨­å®š**     | ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã® API è¨­å®šæƒ…å ± | API Configuration     |
| **æ¥ç¶šç®¡ç†**     | ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ã®æ¥ç¶šçŠ¶æ…‹ç®¡ç†    | Connection Management |

## ğŸ¨ DDD æˆ¦è¡“çš„è¨­è¨ˆ

### 1. ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£

#### è©•ä¾¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£

```python
@dataclass
class Evaluation:
    """
    è©•ä¾¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£

    æƒ…å ±å…¬é–‹æ³•ã«åŸºã¥ãè©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¡¨ç¾ã™ã‚‹ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã€‚
    è©•ä¾¡ã®å®Ÿè¡Œã€çµæœã®ç®¡ç†ã€å±¥æ­´ã®ä¿æŒã‚’æ‹…å½“ã€‚
    """

    id: EvaluationId
    provider_id: ProviderId
    prompt: str
    response: str
    status: EvaluationStatus
    criteria: EvaluationCriteria
    created_at: datetime
    updated_at: datetime
    results: List["EvaluationResult"] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.results is None:
            self.results = []
        if self.metadata is None:
            self.metadata = {}

    def execute_evaluation(self, evaluation_engine: "EvaluationEngine") -> "EvaluationResult":
        """
        è©•ä¾¡ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ç”Ÿæˆã™ã‚‹

        Args:
            evaluation_engine: è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³

        Returns:
            EvaluationResult: è©•ä¾¡çµæœ

        Raises:
            EvaluationError: è©•ä¾¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼
        """
        if self.status != "pending":
            raise EvaluationError(f"Cannot execute evaluation in status: {self.status}")

        self.status = "in_progress"
        self.updated_at = datetime.utcnow()

        try:
            result = evaluation_engine.evaluate(
                prompt=self.prompt,
                response=self.response,
                criteria=self.criteria
            )

            self.results.append(result)
            self.status = "completed"
            self.updated_at = datetime.utcnow()

            return result

        except Exception as e:
            self.status = "failed"
            self.updated_at = datetime.utcnow()
            self.metadata["error"] = str(e)
            raise EvaluationError(f"Evaluation failed: {e}") from e

    def get_latest_result(self) -> Optional["EvaluationResult"]:
        """æœ€æ–°ã®è©•ä¾¡çµæœã‚’å–å¾—"""
        return self.results[-1] if self.results else None

    def is_completed(self) -> bool:
        """è©•ä¾¡ãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.status == "completed"
```

#### è©•ä¾¡æŒ‡æ¨™ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£

```python
@dataclass
class EvaluationMetric:
    """
    è©•ä¾¡æŒ‡æ¨™ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£

    è©•ä¾¡åŸºæº–ã®å®šç¾©ã¨ç®¡ç†ã‚’æ‹…å½“ã€‚
    æƒ…å ±å…¬é–‹æ³•ã®6ã¤ã®ä¸é–‹ç¤ºäº‹ç”±ã‚’å«ã‚€å¤šæ§˜ãªè©•ä¾¡æŒ‡æ¨™ã‚’ã‚µãƒãƒ¼ãƒˆã€‚
    """

    id: str
    name: str
    description: str
    metric_type: MetricType
    weight: float
    scoring_criteria: Dict[int, str]
    evaluation_prompt: str
    evaluation_steps: List[str]
    examples: List[Dict[str, Any]]
    reference_text: Optional[str] = None

    def __post_init__(self):
        self._validate()

    def _validate(self):
        """è©•ä¾¡æŒ‡æ¨™ã®å¦¥å½“æ€§æ¤œè¨¼"""
        if not 0.0 <= self.weight <= 1.0:
            raise ValueError(f"Weight must be between 0.0 and 1.0, got {self.weight}")

        if not all(1 <= score <= 5 for score in self.scoring_criteria.keys()):
            raise ValueError("Scoring criteria must use 1-5 scale")

        if len(self.scoring_criteria) != 5:
            raise ValueError("Must provide criteria for all 5 score levels")

    def evaluate_response(self, prompt: str, response: str, context: Dict[str, Any] = None) -> "MetricScore":
        """
        å¿œç­”ã‚’è©•ä¾¡ã—ã¦ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º

        Args:
            prompt: è©•ä¾¡å¯¾è±¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            response: è©•ä¾¡å¯¾è±¡ã®å¿œç­”
            context: è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±

        Returns:
            MetricScore: è©•ä¾¡ã‚¹ã‚³ã‚¢
        """
        # è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
        pass
```

### 2. å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

#### ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

```python
@dataclass(frozen=True)
class Score:
    """
    è©•ä¾¡ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    1-5ã‚¹ã‚±ãƒ¼ãƒ«ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¾ã€‚
    ä¸å¤‰æ€§ã‚’ä¿è¨¼ã—ã€ã‚¹ã‚³ã‚¢ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã€‚
    """

    value: float
    confidence: float  # 0.0-1.0
    reasoning: str
    metadata: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if not 1.0 <= self.value <= 5.0:
            raise ValueError(f"Score must be between 1.0 and 5.0, got {self.value}")

        if not 0.0 <= self.confidence <= 1.0:
            raise ValueError(f"Confidence must be between 0.0 and 1.0, got {self.confidence}")

        if not self.reasoning.strip():
            raise ValueError("Reasoning cannot be empty")

    def is_high_confidence(self, threshold: float = 0.8) -> bool:
        """é«˜ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.confidence >= threshold

    def to_category(self) -> str:
        """ã‚¹ã‚³ã‚¢ã‚’ã‚«ãƒ†ã‚´ãƒªã«å¤‰æ›"""
        if self.value >= 4.5:
            return "Excellent"
        elif self.value >= 3.5:
            return "Good"
        elif self.value >= 2.5:
            return "Fair"
        elif self.value >= 1.5:
            return "Poor"
        else:
            return "Very Poor"
```

#### è©•ä¾¡åŸºæº–å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

```python
@dataclass(frozen=True)
class EvaluationCriteria:
    """
    è©•ä¾¡åŸºæº–å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹åŸºæº–ã®çµ„ã¿åˆã‚ã›ã‚’è¡¨ç¾ã€‚
    è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã¨é‡ã¿ä»˜ã‘ã‚’ç®¡ç†ã€‚
    """

    metrics: List[str]  # è©•ä¾¡æŒ‡æ¨™ID
    weights: Dict[str, float]  # æŒ‡æ¨™åˆ¥é‡ã¿ä»˜ã‘
    evaluation_type: str  # standard, legal, custom
    parameters: Dict[str, Any]

    def __post_init__(self):
        # é‡ã¿ä»˜ã‘ã®åˆè¨ˆãŒ1.0ã«ãªã‚‹ã“ã¨ã‚’æ¤œè¨¼
        total_weight = sum(self.weights.values())
        if abs(total_weight - 1.0) > 0.001:
            raise ValueError(f"Weights must sum to 1.0, got {total_weight}")

        # å…¨ã¦ã®æŒ‡æ¨™ã«é‡ã¿ä»˜ã‘ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ¤œè¨¼
        if set(self.metrics) != set(self.weights.keys()):
            raise ValueError("All metrics must have corresponding weights")

    def get_weighted_score(self, scores: Dict[str, Score]) -> float:
        """é‡ã¿ä»˜ã‘ã•ã‚ŒãŸç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        total_score = 0.0
        total_weight = 0.0

        for metric_id, score in scores.items():
            if metric_id in self.weights:
                weight = self.weights[metric_id]
                total_score += score.value * weight
                total_weight += weight

        return total_score / total_weight if total_weight > 0 else 0.0
```

### 3. é›†ç´„ãƒ«ãƒ¼ãƒˆ

#### è©•ä¾¡çµæœé›†ç´„

```python
@dataclass
class EvaluationResult:
    """
    è©•ä¾¡çµæœé›†ç´„ãƒ«ãƒ¼ãƒˆ

    è©•ä¾¡ã®çµæœã‚’è¡¨ç¾ã™ã‚‹é›†ç´„ã€‚
    è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã®çµæœã‚’çµ±åˆã—ã€ä¸€è²«æ€§ã‚’ä¿è¨¼ã€‚
    """

    id: EvaluationResultId
    evaluation_id: EvaluationId
    overall_score: Score
    metric_scores: Dict[str, Score]
    evaluation_type: str
    created_at: datetime
    metadata: Dict[str, Any]

    def __post_init__(self):
        self._validate_consistency()

    def _validate_consistency(self):
        """è©•ä¾¡çµæœã®ä¸€è²«æ€§ã‚’æ¤œè¨¼"""
        # ç·åˆã‚¹ã‚³ã‚¢ã¨å€‹åˆ¥ã‚¹ã‚³ã‚¢ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if not self.metric_scores:
            raise ValueError("Metric scores cannot be empty")

        # è©•ä¾¡ã‚¿ã‚¤ãƒ—ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        valid_types = ["standard", "legal", "custom"]
        if self.evaluation_type not in valid_types:
            raise ValueError(f"Invalid evaluation type: {self.evaluation_type}")

    def get_metric_score(self, metric_id: str) -> Optional[Score]:
        """æŒ‡å®šã•ã‚ŒãŸè©•ä¾¡æŒ‡æ¨™ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        return self.metric_scores.get(metric_id)

    def get_failed_metrics(self, threshold: float = 2.0) -> List[str]:
        """å¤±æ•—ã—ãŸè©•ä¾¡æŒ‡æ¨™ã‚’å–å¾—"""
        return [
            metric_id for metric_id, score in self.metric_scores.items()
            if score.value < threshold
        ]

    def is_overall_success(self, threshold: float = 3.0) -> bool:
        """ç·åˆçš„ãªæˆåŠŸåˆ¤å®š"""
        return self.overall_score.value >= threshold
```

### 4. ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹

#### è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³ã‚µãƒ¼ãƒ“ã‚¹

```python
class EvaluationEngine(ABC):
    """
    è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹

    è¤‡é›‘ãªè©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã—ã€è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã‚’çµ±åˆã—ã¦
    æœ€çµ‚çš„ãªè©•ä¾¡çµæœã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """

    @abstractmethod
    async def evaluate(
        self,
        prompt: str,
        response: str,
        criteria: EvaluationCriteria,
        context: Dict[str, Any] = None
    ) -> "EvaluationResult":
        """è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        pass

    @abstractmethod
    async def batch_evaluate(
        self,
        evaluations: List[Evaluation]
    ) -> List["EvaluationResult"]:
        """ãƒãƒƒãƒè©•ä¾¡ã‚’å®Ÿè¡Œ"""
        pass
```

#### æ¨™æº–è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³

```python
class StandardEvaluationEngine(EvaluationEngine):
    """
    æ¨™æº–è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…

    7ã¤ã®æ¨™æº–è©•ä¾¡æŒ‡æ¨™ï¼ˆæ­£ç¢ºæ€§ã€åŒ…æ‹¬æ€§ã€é–¢é€£æ€§ã€æ˜ç¢ºæ€§ã€
    æœ‰ç”¨æ€§ã€ä¸€è²«æ€§ã€é©åˆ‡æ€§ï¼‰ã«ã‚ˆã‚‹è©•ä¾¡ã‚’å®Ÿè¡Œã€‚
    """

    def __init__(self, llm_provider: "LLMProvider", metrics_repository: "MetricsRepository"):
        self._llm_provider = llm_provider
        self._metrics_repository = metrics_repository

    async def evaluate(
        self,
        prompt: str,
        response: str,
        criteria: EvaluationCriteria,
        context: Dict[str, Any] = None
    ) -> "EvaluationResult":
        """
        æ¨™æº–è©•ä¾¡ã®å®Ÿè¡Œ

        Args:
            prompt: è©•ä¾¡å¯¾è±¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            response: è©•ä¾¡å¯¾è±¡ã®å¿œç­”
            criteria: è©•ä¾¡åŸºæº–
            context: è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            EvaluationResult: è©•ä¾¡çµæœ
        """
        metric_scores = {}

        # å„è©•ä¾¡æŒ‡æ¨™ã«å¯¾ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œ
        for metric_id in criteria.metrics:
            metric = await self._metrics_repository.get_by_id(metric_id)
            score = await self._evaluate_metric(prompt, response, metric, context)
            metric_scores[metric_id] = score

        # é‡ã¿ä»˜ã‘ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        weighted_score = criteria.get_weighted_score(metric_scores)

        return EvaluationResult(
            overall_score=weighted_score,
            metric_scores=metric_scores,
            evaluation_type="standard",
            metadata={
                "criteria": criteria,
                "context": context or {}
            }
        )
```

### 5. ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆ

#### è©•ä¾¡ã‚¤ãƒ™ãƒ³ãƒˆ

```python
@dataclass
class EvaluationStarted:
    """è©•ä¾¡é–‹å§‹ã‚¤ãƒ™ãƒ³ãƒˆ"""
    evaluation_id: str
    provider_id: str
    criteria_type: str
    occurred_at: datetime = datetime.utcnow()

    def __post_init__(self):
        self.event_id = f"evaluation_started_{self.evaluation_id}"

@dataclass
class EvaluationCompleted:
    """è©•ä¾¡å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆ"""
    evaluation_id: str
    overall_score: float
    execution_time: float
    occurred_at: datetime = datetime.utcnow()

    def __post_init__(self):
        self.event_id = f"evaluation_completed_{self.evaluation_id}"

@dataclass
class EvaluationFailed:
    """è©•ä¾¡å¤±æ•—ã‚¤ãƒ™ãƒ³ãƒˆ"""
    evaluation_id: str
    error_message: str
    error_type: str
    occurred_at: datetime = datetime.utcnow()

    def __post_init__(self):
        self.event_id = f"evaluation_failed_{self.evaluation_id}"
```

### 6. ãƒªãƒã‚¸ãƒˆãƒªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

#### è©•ä¾¡ãƒªãƒã‚¸ãƒˆãƒª

```python
class EvaluationRepository(ABC):
    """è©•ä¾¡ãƒªãƒã‚¸ãƒˆãƒªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def save(self, evaluation: Evaluation) -> None:
        """è©•ä¾¡ã‚’ä¿å­˜"""
        pass

    @abstractmethod
    async def get_by_id(self, evaluation_id: str) -> Optional[Evaluation]:
        """IDã§è©•ä¾¡ã‚’å–å¾—"""
        pass

    @abstractmethod
    async def get_by_provider(self, provider_id: str) -> List[Evaluation]:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã§è©•ä¾¡ã‚’å–å¾—"""
        pass

    @abstractmethod
    async def get_recent_evaluations(self, limit: int = 100) -> List[Evaluation]:
        """æœ€è¿‘ã®è©•ä¾¡ã‚’å–å¾—"""
        pass

    @abstractmethod
    async def delete(self, evaluation_id: str) -> None:
        """è©•ä¾¡ã‚’å‰Šé™¤"""
        pass
```

## ğŸ”„ ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•è¨­è¨ˆ

### 1. ã‚¤ãƒ™ãƒ³ãƒˆç™ºè¡Œ

```python
class Evaluation:
    """è©•ä¾¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆç™ºè¡Œç‰ˆï¼‰"""

    def execute_evaluation(self, evaluation_engine: "EvaluationEngine") -> "EvaluationResult":
        """è©•ä¾¡ã‚’å®Ÿè¡Œï¼ˆã‚¤ãƒ™ãƒ³ãƒˆç™ºè¡Œï¼‰"""
        # è©•ä¾¡é–‹å§‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ
        DomainEvents.publish(EvaluationStarted(
            evaluation_id=self.id.value,
            provider_id=self.provider_id.value,
            criteria_type=self.criteria.evaluation_type
        ))

        try:
            result = evaluation_engine.evaluate(
                prompt=self.prompt,
                response=self.response,
                criteria=self.criteria
            )

            # è©•ä¾¡å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ
            DomainEvents.publish(EvaluationCompleted(
                evaluation_id=self.id.value,
                overall_score=result.overall_score.value,
                execution_time=result.execution_time
            ))

            return result

        except Exception as e:
            # è©•ä¾¡å¤±æ•—ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ
            DomainEvents.publish(EvaluationFailed(
                evaluation_id=self.id.value,
                error_message=str(e),
                error_type=type(e).__name__
            ))
            raise
```

### 2. ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼

```python
class EvaluationEventHandler:
    """è©•ä¾¡ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self, logger: StructuredLogger, metrics_collector: MetricsCollector):
        self.logger = logger
        self.metrics_collector = metrics_collector

    def handle_evaluation_started(self, event: EvaluationStarted):
        """è©•ä¾¡é–‹å§‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†"""
        self.logger.info(
            "Evaluation started",
            evaluation_id=event.evaluation_id,
            provider_id=event.provider_id,
            criteria_type=event.criteria_type
        )

        self.metrics_collector.increment_counter(
            "evaluation_attempts_total",
            labels={"status": "started", "provider": event.provider_id}
        )

    def handle_evaluation_completed(self, event: EvaluationCompleted):
        """è©•ä¾¡å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†"""
        self.logger.info(
            "Evaluation completed",
            evaluation_id=event.evaluation_id,
            overall_score=event.overall_score,
            execution_time=event.execution_time
        )

        self.metrics_collector.increment_counter(
            "evaluation_attempts_total",
            labels={"status": "completed", "provider": "unknown"}
        )

        self.metrics_collector.record_histogram(
            "evaluation_duration_seconds",
            event.execution_time,
            labels={"evaluation_id": event.evaluation_id}
        )
```

## ğŸ§ª DDD ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### 1. ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ

```python
class TestEvaluation:
    """è©•ä¾¡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ†ã‚¹ãƒˆ"""

    def test_execute_evaluation_success(self):
        """è©•ä¾¡å®Ÿè¡ŒæˆåŠŸã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        evaluation = Evaluation(
            id=EvaluationId.generate(),
            provider_id=ProviderId("openai"),
            prompt="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            response="ãƒ†ã‚¹ãƒˆå¿œç­”",
            status=EvaluationStatus.PENDING,
            criteria=EvaluationCriteria(
                metrics=["accuracy", "relevance"],
                weights={"accuracy": 0.6, "relevance": 0.4},
                evaluation_type="standard",
                parameters={}
            ),
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )

        mock_engine = Mock(spec=EvaluationEngine)
        mock_engine.evaluate.return_value = EvaluationResult(
            id=EvaluationResultId.generate(),
            evaluation_id=evaluation.id,
            overall_score=Score(4.0, 0.9, "Good response"),
            metric_scores={},
            evaluation_type="standard",
            created_at=datetime.utcnow(),
            metadata={}
        )

        # Act
        result = evaluation.execute_evaluation(mock_engine)

        # Assert
        assert result is not None
        assert evaluation.status == EvaluationStatus.COMPLETED
        assert len(evaluation.results) == 1
        mock_engine.evaluate.assert_called_once()

    def test_execute_evaluation_failure(self):
        """è©•ä¾¡å®Ÿè¡Œå¤±æ•—ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        evaluation = Evaluation(...)
        mock_engine = Mock(spec=EvaluationEngine)
        mock_engine.evaluate.side_effect = Exception("Evaluation failed")

        # Act & Assert
        with pytest.raises(EvaluationError):
            evaluation.execute_evaluation(mock_engine)

        assert evaluation.status == EvaluationStatus.FAILED
        assert "error" in evaluation.metadata
```

### 2. å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ†ã‚¹ãƒˆ

```python
class TestScore:
    """ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ†ã‚¹ãƒˆ"""

    def test_valid_score_creation(self):
        """æœ‰åŠ¹ãªã‚¹ã‚³ã‚¢ã®ä½œæˆãƒ†ã‚¹ãƒˆ"""
        score = Score(
            value=4.5,
            confidence=0.9,
            reasoning="æ˜ç¢ºãªæ ¹æ‹ ã«åŸºã¥ãè©•ä¾¡"
        )

        assert score.value == 4.5
        assert score.confidence == 0.9
        assert score.reasoning == "æ˜ç¢ºãªæ ¹æ‹ ã«åŸºã¥ãè©•ä¾¡"

    def test_invalid_score_value_raises_error(self):
        """ç„¡åŠ¹ãªã‚¹ã‚³ã‚¢å€¤ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""
        with pytest.raises(ValueError):
            Score(
                value=6.0,  # ç„¡åŠ¹ãªå€¤
                confidence=0.9,
                reasoning="ç„¡åŠ¹ãªã‚¹ã‚³ã‚¢"
            )

    def test_is_high_confidence(self):
        """é«˜ä¿¡é ¼åº¦åˆ¤å®šã®ãƒ†ã‚¹ãƒˆ"""
        high_confidence_score = Score(
            value=4.0,
            confidence=0.9,
            reasoning="é«˜ä¿¡é ¼åº¦ã®ã‚¹ã‚³ã‚¢"
        )

        low_confidence_score = Score(
            value=4.0,
            confidence=0.6,
            reasoning="ä½ä¿¡é ¼åº¦ã®ã‚¹ã‚³ã‚¢"
        )

        assert high_confidence_score.is_high_confidence()
        assert not low_confidence_score.is_high_confidence()
```

## ğŸ“Š DDD å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£è¨­è¨ˆåŸå‰‡

- **ä¸€æ„æ€§**: å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯ä¸€æ„ã® ID ã‚’æŒã¤
- **ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«**: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®çŠ¶æ…‹é·ç§»ã‚’æ˜ç¢ºã«å®šç¾©
- **ä¸å¤‰æ¡ä»¶**: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ•´åˆæ€§ã‚’ä¿ã¤ä¸å¤‰æ¡ä»¶ã‚’å®šç¾©

### 2. å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¨­è¨ˆåŸå‰‡

- **ä¸å¤‰æ€§**: å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ä¸å¤‰ã§ã‚ã‚‹
- **ç­‰ä¾¡æ€§**: å€¤ã«ã‚ˆã‚‹ç­‰ä¾¡æ€§åˆ¤å®š
- **è‡ªå·±æ¤œè¨¼**: ä½œæˆæ™‚ã«å¦¥å½“æ€§ã‚’æ¤œè¨¼

### 3. é›†ç´„è¨­è¨ˆåŸå‰‡

- **ä¸€è²«æ€§**: é›†ç´„å†…ã®ä¸€è²«æ€§ã‚’ä¿ã¤
- **å¢ƒç•Œ**: é©åˆ‡ãªé›†ç´„å¢ƒç•Œã‚’è¨­å®š
- **å‚ç…§**: é›†ç´„é–“ã®å‚ç…§ã¯ ID ã«ã‚ˆã‚‹

### 4. ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹è¨­è¨ˆåŸå‰‡

- **ç„¡çŠ¶æ…‹**: ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ã‚¹ã¯ç„¡çŠ¶æ…‹ã§ã‚ã‚‹
- **ç´”ç²‹é–¢æ•°**: å‰¯ä½œç”¨ã®ãªã„ç´”ç²‹é–¢æ•°ã¨ã—ã¦è¨­è¨ˆ
- **ãƒ†ã‚¹ãƒˆå®¹æ˜“æ€§**: ãƒ¢ãƒƒã‚¯åŒ–ã—ã‚„ã™ã„è¨­è¨ˆ

---

_ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³é§†å‹•è¨­è¨ˆã«ã‚ˆã‚Šã€Disclosure Evaluator ã¯è¤‡é›‘ãªãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©åˆ‡ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€ä¿å®ˆæ€§ã®é«˜ã„ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¾ã™ã€‚_
