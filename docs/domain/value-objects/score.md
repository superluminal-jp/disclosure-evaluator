# ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¨­è¨ˆ

## ğŸ“‹ æ–‡æ›¸æƒ…å ±

| é …ç›®       | å†…å®¹                     |
| ---------- | ------------------------ |
| æ–‡æ›¸å     | ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¨­è¨ˆ |
| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | 1.0                      |
| ä½œæˆæ—¥     | 2025 å¹´ 9 æœˆ 28 æ—¥       |
| ä½œæˆè€…     | AI é–‹ç™ºãƒãƒ¼ãƒ             |
| æ‰¿èªè€…     | æŠ€è¡“è²¬ä»»è€…               |
| ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ | è‰æ¡ˆ                     |

## ğŸ¯ æ¦‚è¦

ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€1-5 ã‚¹ã‚±ãƒ¼ãƒ«ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¾ã™ã‚‹ä¸å¤‰ã®å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ã‚¹ã‚³ã‚¢ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã—ã€ä¿¡é ¼åº¦ã¨æ¨è«–ã‚’å«ã‚€åŒ…æ‹¬çš„ãªè©•ä¾¡æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸ—ï¸ å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¨­è¨ˆ

### 1. åŸºæœ¬æ§‹é€ 

```python
@dataclass(frozen=True)
class Score:
    """
    è©•ä¾¡ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    1-5ã‚¹ã‚±ãƒ¼ãƒ«ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¾ã€‚
    ä¸å¤‰æ€§ã‚’ä¿è¨¼ã—ã€ã‚¹ã‚³ã‚¢ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã€‚
    """

    value: float
    confidence: float  # 0.0-1.0
    reasoning: str
    metadata: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if not 1.0 <= self.value <= 5.0:
            raise ValueError(f"Score must be between 1.0 and 5.0, got {self.value}")

        if not 0.0 <= self.confidence <= 1.0:
            raise ValueError(f"Confidence must be between 0.0 and 1.0, got {self.confidence}")

        if not self.reasoning.strip():
            raise ValueError("Reasoning cannot be empty")
```

### 2. å±æ€§èª¬æ˜

| å±æ€§å         | å‹                       | èª¬æ˜                     |
| -------------- | ------------------------ | ------------------------ |
| **value**      | float                    | 1-5 ã‚¹ã‚±ãƒ¼ãƒ«ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ |
| **confidence** | float                    | 0.0-1.0 ã®ä¿¡é ¼åº¦         |
| **reasoning**  | str                      | è©•ä¾¡ç†ç”±ã®è©³ç´°èª¬æ˜       |
| **metadata**   | Optional[Dict[str, Any]] | è¿½åŠ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿         |

## ğŸ”§ ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯

### 1. ä¿¡é ¼åº¦åˆ¤å®š

```python
def is_high_confidence(self, threshold: float = 0.8) -> bool:
    """é«˜ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‹ãƒã‚§ãƒƒã‚¯"""
    return self.confidence >= threshold

def is_low_confidence(self, threshold: float = 0.5) -> bool:
    """ä½ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‹ãƒã‚§ãƒƒã‚¯"""
    return self.confidence < threshold

def get_confidence_level(self) -> str:
    """ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—"""
    if self.confidence >= 0.9:
        return "Very High"
    elif self.confidence >= 0.8:
        return "High"
    elif self.confidence >= 0.6:
        return "Medium"
    elif self.confidence >= 0.4:
        return "Low"
    else:
        return "Very Low"
```

### 2. ã‚¹ã‚³ã‚¢åˆ†é¡

```python
def to_category(self) -> str:
    """ã‚¹ã‚³ã‚¢ã‚’ã‚«ãƒ†ã‚´ãƒªã«å¤‰æ›"""
    if self.value >= 4.5:
        return "Excellent"
    elif self.value >= 3.5:
        return "Good"
    elif self.value >= 2.5:
        return "Fair"
    elif self.value >= 1.5:
        return "Poor"
    else:
        return "Very Poor"

def to_grade(self) -> str:
    """ã‚¹ã‚³ã‚¢ã‚’ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¤‰æ›"""
    if self.value >= 4.5:
        return "A"
    elif self.value >= 3.5:
        return "B"
    elif self.value >= 2.5:
        return "C"
    elif self.value >= 1.5:
        return "D"
    else:
        return "F"
```

### 3. æ¯”è¼ƒæ¼”ç®—

```python
def __lt__(self, other: 'Score') -> bool:
    """ã‚¹ã‚³ã‚¢ã®å¤§å°æ¯”è¼ƒï¼ˆå€¤ã®ã¿ï¼‰"""
    if not isinstance(other, Score):
        return NotImplemented
    return self.value < other.value

def __le__(self, other: 'Score') -> bool:
    """ã‚¹ã‚³ã‚¢ã®å¤§å°æ¯”è¼ƒï¼ˆå€¤ã®ã¿ï¼‰"""
    if not isinstance(other, Score):
        return NotImplemented
    return self.value <= other.value

def __gt__(self, other: 'Score') -> bool:
    """ã‚¹ã‚³ã‚¢ã®å¤§å°æ¯”è¼ƒï¼ˆå€¤ã®ã¿ï¼‰"""
    if not isinstance(other, Score):
        return NotImplemented
    return self.value > other.value

def __ge__(self, other: 'Score') -> bool:
    """ã‚¹ã‚³ã‚¢ã®å¤§å°æ¯”è¼ƒï¼ˆå€¤ã®ã¿ï¼‰"""
    if not isinstance(other, Score):
        return NotImplemented
    return self.value >= other.value

def __eq__(self, other: 'Score') -> bool:
    """ã‚¹ã‚³ã‚¢ã®ç­‰ä¾¡æ€§æ¯”è¼ƒï¼ˆå€¤ã®ã¿ï¼‰"""
    if not isinstance(other, Score):
        return NotImplemented
    return abs(self.value - other.value) < 0.01

def __ne__(self, other: 'Score') -> bool:
    """ã‚¹ã‚³ã‚¢ã®éç­‰ä¾¡æ€§æ¯”è¼ƒï¼ˆå€¤ã®ã¿ï¼‰"""
    return not self.__eq__(other)
```

### 4. çµ±è¨ˆæ¼”ç®—

```python
@classmethod
def average(cls, scores: List['Score']) -> 'Score':
    """è¤‡æ•°ã‚¹ã‚³ã‚¢ã®å¹³å‡ã‚’è¨ˆç®—"""
    if not scores:
        raise ValueError("Cannot calculate average of empty list")

    total_value = sum(score.value for score in scores)
    total_confidence = sum(score.confidence for score in scores)
    avg_value = total_value / len(scores)
    avg_confidence = total_confidence / len(scores)

    # æ¨è«–ã‚’çµ±åˆ
    combined_reasoning = f"Average of {len(scores)} scores: " + \
                        " | ".join(score.reasoning[:50] + "..." if len(score.reasoning) > 50
                                  else score.reasoning for score in scores)

    return cls(
        value=avg_value,
        confidence=avg_confidence,
        reasoning=combined_reasoning,
        metadata={"calculation_type": "average", "count": len(scores)}
    )

@classmethod
def weighted_average(cls, scores: List[Tuple['Score', float]]) -> 'Score':
    """é‡ã¿ä»˜ã‘å¹³å‡ã‚’è¨ˆç®—"""
    if not scores:
        raise ValueError("Cannot calculate weighted average of empty list")

    total_weight = sum(weight for _, weight in scores)
    if abs(total_weight - 1.0) > 0.001:
        raise ValueError(f"Weights must sum to 1.0, got {total_weight}")

    weighted_value = sum(score.value * weight for score, weight in scores)
    weighted_confidence = sum(score.confidence * weight for score, weight in scores)

    # é‡ã¿ä»˜ã‘æ¨è«–ã‚’çµ±åˆ
    combined_reasoning = f"Weighted average of {len(scores)} scores: " + \
                        " | ".join(f"{score.reasoning[:30]}... (w:{weight:.2f})"
                                  for score, weight in scores)

    return cls(
        value=weighted_value,
        confidence=weighted_confidence,
        reasoning=combined_reasoning,
        metadata={"calculation_type": "weighted_average", "count": len(scores)}
    )
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆè¨­è¨ˆ

### 1. å˜ä½“ãƒ†ã‚¹ãƒˆ

```python
class TestScore:
    """ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ†ã‚¹ãƒˆ"""

    def test_valid_score_creation(self):
        """æœ‰åŠ¹ãªã‚¹ã‚³ã‚¢ã®ä½œæˆãƒ†ã‚¹ãƒˆ"""
        score = Score(
            value=4.5,
            confidence=0.9,
            reasoning="æ˜ç¢ºãªæ ¹æ‹ ã«åŸºã¥ãè©•ä¾¡"
        )

        assert score.value == 4.5
        assert score.confidence == 0.9
        assert score.reasoning == "æ˜ç¢ºãªæ ¹æ‹ ã«åŸºã¥ãè©•ä¾¡"

    def test_invalid_score_value_raises_error(self):
        """ç„¡åŠ¹ãªã‚¹ã‚³ã‚¢å€¤ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""
        with pytest.raises(ValueError):
            Score(
                value=6.0,  # ç„¡åŠ¹ãªå€¤
                confidence=0.9,
                reasoning="ç„¡åŠ¹ãªã‚¹ã‚³ã‚¢"
            )

    def test_invalid_confidence_raises_error(self):
        """ç„¡åŠ¹ãªä¿¡é ¼åº¦ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""
        with pytest.raises(ValueError):
            Score(
                value=4.0,
                confidence=1.5,  # ç„¡åŠ¹ãªå€¤
                reasoning="ç„¡åŠ¹ãªä¿¡é ¼åº¦"
            )

    def test_empty_reasoning_raises_error(self):
        """ç©ºã®ç†ç”±ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""
        with pytest.raises(ValueError):
            Score(
                value=4.0,
                confidence=0.9,
                reasoning=""  # ç©ºã®ç†ç”±
            )

    def test_is_high_confidence(self):
        """é«˜ä¿¡é ¼åº¦åˆ¤å®šã®ãƒ†ã‚¹ãƒˆ"""
        high_confidence_score = Score(
            value=4.0,
            confidence=0.9,
            reasoning="é«˜ä¿¡é ¼åº¦ã®ã‚¹ã‚³ã‚¢"
        )

        low_confidence_score = Score(
            value=4.0,
            confidence=0.6,
            reasoning="ä½ä¿¡é ¼åº¦ã®ã‚¹ã‚³ã‚¢"
        )

        assert high_confidence_score.is_high_confidence()
        assert not low_confidence_score.is_high_confidence()

    def test_to_category(self):
        """ã‚¹ã‚³ã‚¢ã‚«ãƒ†ã‚´ãƒªå¤‰æ›ã®ãƒ†ã‚¹ãƒˆ"""
        excellent_score = Score(value=4.8, confidence=0.9, reasoning="å„ªç§€")
        good_score = Score(value=4.0, confidence=0.8, reasoning="è‰¯å¥½")
        fair_score = Score(value=3.0, confidence=0.7, reasoning="æ™®é€š")
        poor_score = Score(value=2.0, confidence=0.6, reasoning="ä¸è‰¯")
        very_poor_score = Score(value=1.2, confidence=0.5, reasoning="éå¸¸ã«ä¸è‰¯")

        assert excellent_score.to_category() == "Excellent"
        assert good_score.to_category() == "Good"
        assert fair_score.to_category() == "Fair"
        assert poor_score.to_category() == "Poor"
        assert very_poor_score.to_category() == "Very Poor"

    def test_comparison_operators(self):
        """æ¯”è¼ƒæ¼”ç®—å­ã®ãƒ†ã‚¹ãƒˆ"""
        score1 = Score(value=3.0, confidence=0.8, reasoning="ã‚¹ã‚³ã‚¢1")
        score2 = Score(value=4.0, confidence=0.9, reasoning="ã‚¹ã‚³ã‚¢2")
        score3 = Score(value=3.0, confidence=0.7, reasoning="ã‚¹ã‚³ã‚¢3")

        assert score1 < score2
        assert score2 > score1
        assert score1 <= score2
        assert score2 >= score1
        assert score1 == score3  # å€¤ãŒåŒã˜
        assert score1 != score2

    def test_average_calculation(self):
        """å¹³å‡è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ"""
        scores = [
            Score(value=3.0, confidence=0.8, reasoning="ã‚¹ã‚³ã‚¢1"),
            Score(value=4.0, confidence=0.9, reasoning="ã‚¹ã‚³ã‚¢2"),
            Score(value=5.0, confidence=0.7, reasoning="ã‚¹ã‚³ã‚¢3")
        ]

        avg_score = Score.average(scores)

        assert avg_score.value == 4.0  # (3+4+5)/3
        assert avg_score.confidence == 0.8  # (0.8+0.9+0.7)/3
        assert "Average of 3 scores" in avg_score.reasoning
        assert avg_score.metadata["calculation_type"] == "average"
        assert avg_score.metadata["count"] == 3

    def test_weighted_average_calculation(self):
        """é‡ã¿ä»˜ã‘å¹³å‡è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ"""
        scores_with_weights = [
            (Score(value=3.0, confidence=0.8, reasoning="ã‚¹ã‚³ã‚¢1"), 0.3),
            (Score(value=4.0, confidence=0.9, reasoning="ã‚¹ã‚³ã‚¢2"), 0.5),
            (Score(value=5.0, confidence=0.7, reasoning="ã‚¹ã‚³ã‚¢3"), 0.2)
        ]

        weighted_avg = Score.weighted_average(scores_with_weights)

        expected_value = 3.0 * 0.3 + 4.0 * 0.5 + 5.0 * 0.2  # 3.9
        expected_confidence = 0.8 * 0.3 + 0.9 * 0.5 + 0.7 * 0.2  # 0.83

        assert abs(weighted_avg.value - expected_value) < 0.01
        assert abs(weighted_avg.confidence - expected_confidence) < 0.01
        assert "Weighted average of 3 scores" in weighted_avg.reasoning
        assert weighted_avg.metadata["calculation_type"] == "weighted_average"

    def test_immutability(self):
        """ä¸å¤‰æ€§ã®ãƒ†ã‚¹ãƒˆ"""
        score = Score(value=4.0, confidence=0.8, reasoning="ãƒ†ã‚¹ãƒˆ")

        # å±æ€§ã®å¤‰æ›´ã‚’è©¦è¡Œï¼ˆfrozen=Trueã«ã‚ˆã‚Šå¤±æ•—ã™ã‚‹ã¯ãšï¼‰
        with pytest.raises(AttributeError):
            score.value = 5.0

        with pytest.raises(AttributeError):
            score.confidence = 0.9

        with pytest.raises(AttributeError):
            score.reasoning = "å¤‰æ›´ã•ã‚ŒãŸç†ç”±"
```

### 2. çµ±åˆãƒ†ã‚¹ãƒˆ

```python
class TestScoreIntegration:
    """ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆ"""

    def test_score_in_evaluation_result(self):
        """è©•ä¾¡çµæœã§ã®ã‚¹ã‚³ã‚¢ä½¿ç”¨ãƒ†ã‚¹ãƒˆ"""
        score = Score(
            value=4.2,
            confidence=0.85,
            reasoning="åŒ…æ‹¬çš„ã§æ­£ç¢ºãªå›ç­”",
            metadata={"metric": "accuracy", "provider": "openai"}
        )

        evaluation_result = EvaluationResult(
            id=EvaluationResultId.generate(),
            evaluation_id=EvaluationId.generate(),
            overall_score=score,
            metric_scores={"accuracy": score},
            evaluation_type="standard",
            created_at=datetime.utcnow(),
            metadata={}
        )

        assert evaluation_result.overall_score == score
        assert evaluation_result.get_metric_score("accuracy") == score
        assert evaluation_result.is_overall_success(threshold=4.0)

    def test_score_aggregation(self):
        """ã‚¹ã‚³ã‚¢é›†ç´„ã®ãƒ†ã‚¹ãƒˆ"""
        scores = [
            Score(value=3.5, confidence=0.8, reasoning="æ­£ç¢ºæ€§: è‰¯å¥½"),
            Score(value=4.0, confidence=0.9, reasoning="é–¢é€£æ€§: å„ªç§€"),
            Score(value=3.0, confidence=0.7, reasoning="æ˜ç¢ºæ€§: æ™®é€š")
        ]

        # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        avg_score = Score.average(scores)

        # é‡ã¿ä»˜ã‘å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        weighted_scores = [
            (scores[0], 0.4),  # æ­£ç¢ºæ€§: 40%
            (scores[1], 0.4),  # é–¢é€£æ€§: 40%
            (scores[2], 0.2)   # æ˜ç¢ºæ€§: 20%
        ]
        weighted_avg = Score.weighted_average(weighted_scores)

        assert avg_score.value == 3.5  # (3.5+4.0+3.0)/3
        assert weighted_avg.value == 3.5 * 0.4 + 4.0 * 0.4 + 3.0 * 0.2  # 3.6
        assert avg_score.confidence == 0.8  # (0.8+0.9+0.7)/3
```

## ğŸ”§ å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### 1. ä¸å¤‰æ€§ã®ä¿è¨¼

- **frozen=True**: dataclass ã® frozen å±æ€§ã‚’ä½¿ç”¨
- **æ¤œè¨¼**: ä½œæˆæ™‚ã®å¦¥å½“æ€§æ¤œè¨¼
- **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: ç„¡åŠ¹ãªå€¤ã§ã®ä¾‹å¤–ç™ºç”Ÿ

### 2. ç­‰ä¾¡æ€§ã®å®Ÿè£…

- **å€¤ãƒ™ãƒ¼ã‚¹**: å€¤ã«ã‚ˆã‚‹ç­‰ä¾¡æ€§åˆ¤å®š
- **ç²¾åº¦è€ƒæ…®**: æµ®å‹•å°æ•°ç‚¹ã®ç²¾åº¦ã‚’è€ƒæ…®
- **ãƒãƒƒã‚·ãƒ¥**: ãƒãƒƒã‚·ãƒ¥å¯èƒ½ãªå®Ÿè£…

### 3. æ¯”è¼ƒæ¼”ç®—ã®å®Ÿè£…

- **ä¸€è²«æ€§**: æ¯”è¼ƒæ¼”ç®—ã®ä¸€è²«æ€§
- **å‹å®‰å…¨æ€§**: å‹ãƒã‚§ãƒƒã‚¯ã®å®Ÿè£…
- **NotImplemented**: æœªå¯¾å¿œå‹ã§ã®é©åˆ‡ãªå‡¦ç†

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è€ƒæ…®äº‹é …

### 1. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

- **ä¸å¤‰æ€§**: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å…±æœ‰ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºåˆ¶é™
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: é »ç¹ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚¹ã‚³ã‚¢ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥

### 2. è¨ˆç®—åŠ¹ç‡

- **çµ±è¨ˆæ¼”ç®—**: å¤§é‡ã‚¹ã‚³ã‚¢ã®åŠ¹ç‡çš„ãªé›†ç´„
- **æ¯”è¼ƒæ¼”ç®—**: é«˜é€Ÿãªæ¯”è¼ƒæ¼”ç®—ã®å®Ÿè£…
- **æ–‡å­—åˆ—å‡¦ç†**: æ¨è«–æ–‡å­—åˆ—ã®åŠ¹ç‡çš„ãªå‡¦ç†

## ğŸ”„ å°†æ¥ã®æ‹¡å¼µæ€§

### 1. æ–°ã—ã„çµ±è¨ˆæ¼”ç®—

- **ä¸­å¤®å€¤**: ä¸­å¤®å€¤ã®è¨ˆç®—
- **æ¨™æº–åå·®**: åˆ†æ•£ã®è¨ˆç®—
- **åˆ†ä½æ•°**: åˆ†ä½æ•°ã®è¨ˆç®—

### 2. é«˜åº¦ãªåˆ†ææ©Ÿèƒ½

- **ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ**: æ™‚ç³»åˆ—ã§ã®ã‚¹ã‚³ã‚¢å¤‰åŒ–åˆ†æ
- **ç•°å¸¸æ¤œå‡º**: ç•°å¸¸ãªã‚¹ã‚³ã‚¢ã®æ¤œå‡º
- **äºˆæ¸¬**: ã‚¹ã‚³ã‚¢ã®äºˆæ¸¬æ©Ÿèƒ½

### 3. å¯è¦–åŒ–æ©Ÿèƒ½

- **ã‚°ãƒ©ãƒ•ç”Ÿæˆ**: ã‚¹ã‚³ã‚¢ã®å¯è¦–åŒ–
- **ãƒ¬ãƒãƒ¼ãƒˆ**: ã‚¹ã‚³ã‚¢ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
- **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

---

_ã“ã®ã‚¹ã‚³ã‚¢å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¨­è¨ˆã«ã‚ˆã‚Šã€Disclosure Evaluator ã¯æ­£ç¢ºã§ä¿¡é ¼æ€§ã®é«˜ã„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿç¾ã—ã¾ã™ã€‚_
